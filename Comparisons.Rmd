---
title: "Comparisons"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---


<body style="background-color:aliceblue;">

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```


# Scalability and Tuning of parameters 

Initially, the creators aimed to assess various parameters, such as incremental frequency computation, incremental solving, similarity lower bounding, in-order feature selection, dynamic node selection, and dataset-based cache. During the tuning process, the algorithm underwent evaluation based on runtime performance and various metrics. Benchmark results were analyzed, and settings, where the differences were deemed insignificant, were excluded from further consideration in the tuning studies.


## Caching Strategy 
For the varying of the cache strategy which is an optimization technique used to speed up computer programs by using the data storage of expensive function calls and returning them when the same inputs are encountered again. (Memoisation) And for each combination keep a consistent depth of depth of 3 to 4 and task the algorithms to compute fifteen optimal decision trees, one tree for each value of n âˆˆ [1, 15]. After tuning the parameters, it became evident that the inclusion of the cache dataset significantly improved performance. Interestingly, branch caching demonstrated a nuanced advantage over cache entries, showcasing a subtle yet discernible difference in performance metrics. This observation underscores the importance of carefully considering caching strategies to optimize algorithmic efficiency and overall computational outcomes.

## Lower Bound Similarity and Incremental Frequency 
Activating lower bound similarity and incremental frequency computation in this configuration retained the same parameters as the previous setting, with a similar exclusion of insignificant benchmarks. The runtime analysis reaffirmed the importance of incremental computation over recomputation from scratch, highlighting its role in enhancing computational efficiency. Additionally, setting a lower bound contributed to reduced runtimes, emphasizing its significance in streamlining the computational process and optimizing performance.


## Node Selection and Feature Selection Strategy 
Using the Gini coefficients the algorithm is run now including varying the feature selection strategy and the node selection strategy which is either post-order or dynamic. I FEEL LIKE WE PUT WHAT WE LEARNED AND PERFORMED IN CLASS HERE!!! Since we have previously evaluated and adjusted the lower bound and incremental frequency parameters, setting these parameters appropriately demonstrates that random feature selection results in longer computational times due to the additional overhead of exploring a larger feature space. Dynamic node selection is consistently but minimally better to a fixed post-order selection because it allows for adaptability and responsiveness to the evolving structure of the decision tree. The dynamic node selection can adjust its strategy based on real-time information, leading to more efficient and effective decision-making processes.

## Binary Features and Tree Depth impact 
In terms of scalability, the algorithm's runtime is most significantly impacted by the depth of the tree, with the number of binary features also playing a notable role. However, drawing conclusive statements on the impact of the number of features on runtime is challenging due to the pruning mechanism inherent in the MurTree algorithm. Since pruning largely depends on the dataset structure, duplicating features would not provide definitive insights into their impact on runtime. Instead, it is more appropriate to consider the computational complexity of our algorithm and the number of possible decision trees as indicative measures of the influence of the number of binary features and the sparsity of the feature vectors on runtime.

  
# DL8.5 vs. MurTree

2018 | 7 | 23 Last compiled: `r Sys.Date()`

I'm writing this tutorial going from the top down. And, this is how it will be printed. So, notice the second post is second in the list. If you want your most recent post to be at the top, then make a new post starting at the top. If you want the oldest first, do, then keep adding to the bottom

# Random FOrests and MurTree

So far this is just a blog where you can write in plain text and serve your writing to a webpage. One of the main purposes of this lab journal is to record your progress learning R. The reason I am asking you to use this process is because you can both make a website, and a lab journal, and learn R all in R-studio. This makes everything really convenient and in the sam place. 

So, let's say you are learning how to make a histogram in R. For example, maybe you want to sample 100 numbers from a normal distribution with mean = 0, and standard deviation =1, and then you want to plot a histogram. You can do this right here by using an r code block, like this:

```{r}
samples <- rnorm(100, mean=0, sd=1)
hist(samples)
```

When you knit this R Markdown document, you will see that the histogram is printed to the page, along with the R code. This document can be set up to hide the R code in the webpage, just delete the comment (hashtag), from the cold folding option in the yaml header up top. For purposes of letting yourself see the code, and me see the code, best to keep it the way that it is. You learn all of these things and more can be customized in each R code block.

# The big idea

Use this lab journal to record what you do in R. This way I will be able to see what you are doing and help you along the way. You will also be creating a repository of all the things you do. You can make posts about everything. Learning specific things in R (project unrelated), and doing things for the project that we will discuss at the beginning of the Fall semester. You can get started now by fiddling around with googling things, and trying stuff out in R. I've placed some helpful starting links in the links page on this website

# What can you do right now by yourself?

It's hard to learn programming when you don't have specific problems that you are trying to solve. Everything just seems abstract.

I wrote an [introductory programming book that introduces R](https://crumplab.github.io/programmingforpsych/), and gives some [concrete problems for you to solve](https://crumplab.github.io/programmingforpsych/programming-challenges-i-learning-the-fundamentals.html). 

To get the hang of journaling and solving the problems to learn programming, my suggestion is that you use this .Rmd file to solve the problems. It would look like this:

# Problem 1

Do simple math with numbers, addition, subtraction, multiplication, division

```{r}
1+2
2*5
5/3
(1+6+4)/5

```

# Problem 2

Put numbers into variables, do simple math on the variables

```{r}
a<-1
b<-2
a+b

d<-c(1,2,3)
e<-c(5,6,7)
d+e
d*e
d/e

```

# Problem 3

Write code that will place the numbers 1 to 100 separately into a variable using for loop. Then, again using the seq function.

```{r}
# for loop solution
# i becomes the number 1 to 100 at each step of the loop


a <- length(100) # make empty variable, set length to 100
for (i in 1:100){
  a[i] <-i #assigns the number in i, to the ith index of a
}

print(a)

# for loop solution #2

a<-c() #create empty variable using combine command
for (i in 1:100){
  a<-c(a,i) # keeps combining a with itself and the new number in i
}
print(a)

# seq solution
a <- seq(1,100,1) # look up help for seq using ?seq() in console
print(a)

```

# Replace this with problem 4

And keep going. Try to solve the problems with different scripts that provide the same solution. Good luck, Happy coding.
