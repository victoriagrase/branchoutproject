<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Comparisons.knit</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Optimization Trees Review</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">About Decision Trees</a>
</li>
<li>
  <a href="Journal.html">MurTree </a>
</li>
<li>
  <a href="Comparisons.html">MurTree vs. Others</a>
</li>
<li>
  <a href="Links.html">Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<body style="background-color:aliceblue;">
<div id="scalability-and-tuning-of-parameters" class="section level1">
<h1><strong>Scalability and Tuning of parameters</strong></h1>
<p>Initially, the creators aimed to assess various parameters, such as
incremental frequency computation, incremental solving, similarity lower
bounding, in-order feature selection, dynamic node selection, and
dataset-based cache. During the tuning process, the algorithm underwent
evaluation based on runtime performance and various metrics. Benchmark
results were analyzed, and settings, where the differences were deemed
insignificant, were excluded from further consideration in the tuning
studies.</p>
<div id="caching-strategy" class="section level2">
<h2><strong>Caching Strategy</strong></h2>
<p>For the varying of the cache strategy which is an optimization
technique used to speed up computer programs by using the data storage
of expensive function calls and returning them when the same inputs are
encountered again (Memoisation) and for each combination keep a
consistent depth of depth of 3 to 4 and task the algorithms to compute
fifteen optimal decision trees, one tree for each value of n âˆˆ [1, 15].
After tuning the parameters, it became evident that the inclusion of the
cache dataset significantly improved performance. Interestingly, branch
caching demonstrated a nuanced advantage over cache entries, showcasing
a subtle yet discernible difference in performance metrics. This
observation underscores the importance of carefully considering caching
strategies to optimize algorithmic efficiency and overall computational
outcomes. The provided illustration below gives an example of a caching
strategy used in decision tree which results in quicker runtimes.</p>
<html>
<head>
<style>
        .image3{
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 700px;
            height: 500px;
        }
    </style>
</head>
<body>
<p><img
src="images/caching.png"
class="image3" /></p>
</div>
<div id="lower-bound-similarity-and-incremental-frequency"
class="section level2">
<h2><strong>Lower Bound Similarity and Incremental
Frequency</strong></h2>
<p>Activating lower bound similarity and incremental frequency
computation in this configuration retained the same parameters as the
previous setting, with a similar exclusion of insignificant benchmarks.
The runtime analysis reaffirmed the importance of incremental
computation over recomputation from scratch, highlighting its role in
enhancing computational efficiency. Additionally, setting a lower bound
contributed to reduced runtimes, emphasizing its significance in
streamlining the computational process and optimizing performance.</p>
</div>
<div id="node-selection-and-feature-selection-strategy"
class="section level2">
<h2><strong>Node Selection and Feature Selection Strategy</strong></h2>
<p>Using the Gini coefficients the algorithm is run now including
varying the feature selection strategy and the node selection strategy
which is either post-order or dynamic. Since the creators have
previously evaluated and adjusted the lower bound and incremental
frequency parameters, setting these parameters appropriately
demonstrates that random feature selection results in longer
computational times due to the additional overhead of exploring a larger
feature space. Dynamic node selection is consistently but minimally
better to a fixed post-order selection because it allows for
adaptability and responsiveness to the evolving structure of the
decision tree. The dynamic node selection can adjust its strategy based
on real-time information, leading to more efficient and effective
decision-making processes.</p>
</div>
<div id="binary-features-and-tree-depth-impact" class="section level2">
<h2><strong>Binary Features and Tree Depth impact</strong></h2>
<p>In terms of scalability, the algorithmâ€™s runtime is most
significantly impacted by the depth of the tree, with the number of
binary features also playing a notable role. However, drawing conclusive
statements on the impact of the number of features on runtime is
challenging due to the pruning mechanism inherent in the MurTree
algorithm. Since pruning largely depends on the dataset structure,
duplicating features would not provide definitive insights into their
impact on runtime. Instead, it is more appropriate to consider the
computational complexity of our algorithm and the number of possible
decision trees as indicative measures of the influence of the number of
binary features and the sparsity of the feature vectors on runtime.</p>
</div>
</div>
<div id="optimal-method-comparisons" class="section level1">
<h1><strong>Optimal Method Comparisons</strong></h1>
<div id="sat-method-vs.-murtree" class="section level2">
<h2><strong>SAT method vs.Â MurTree</strong></h2>
<p>The SAT method differs from the MurTree algorithm in optimizing
decision trees. While SAT aims to construct the smallest decision tree
with zero misclassification by subsampling datasets, MurTree directly
minimizes misclassifications without subsampling. It efficiently
computes optimal decision trees even on the same subsampled data used in
the SAT paper, and it can optimize directly with complete datasets. The
discrepancy in runtime between MurTree and SAT is due to MurTreeâ€™s
specialized procedure leveraging classification tree properties. Since
MurTree does not utilize such extendability with new constraints, no
further comparison is made between this method.</p>
</div>
<div id="osdt-method-vs.murtree" class="section level2">
<h2><strong>OSDT method vs.MurTree</strong></h2>
<p>A direct comparison between MurTree and OSDT using a ten-minute
timeout with depth-four trees was experimented with. MurTree efficiently
computed optimal trees with the specified objective within seconds,
while OSDT may require minutes or even timeout. For the majority of
benchmarks, OSDT timeouts unless the sparsity coefficient is set to be
sufficiently high. As a result of increasing the spasticity coefficient,
the computation time required to optimize the decision tree decreases
because the algorithm doesnâ€™t need to explore as many potential
branches. MurTree, on the other hand, handled any sparsity coefficient
within the time limit for trees with a maximum depth of four.</p>
</div>
<div id="gosdt-method-vs.-murtree" class="section level2">
<h2><strong>GOSDT method vs.Â MurTree</strong></h2>
<p>The comparison with GOSDT varies because controlling the depth or
number of nodes isnâ€™t directly supported; instead, the treeâ€™s structure
is influenced by the objective function and the sparsity coefficient.
Results showed that GOSDT timed out on 65% of the datasets. Despite the
absence of depth limitation, approximately 90% of the computed trees had
a depth of at most four, with the maximum depth being seven, and all
trees possessed a small number of nodes. These outcomes align with
GOSDTâ€™s aim of generating compact trees. However, the MurTree approach
can swiftly generate optimal compact trees with any node count within
seconds or minutes, even when the sparsity coefficient is zero. Notably,
after executing the algorithm with a zero sparsity coefficient, the
cache is populated, causing immediate extraction of a tree for any
sparsity value. Essentially, the zero-case coefficient represents the
worst-case scenario, and optimizing with higher sparsity coefficient
values proves advantageous as it enables pruning.</p>
</div>
</div>
<div id="dl8.5-vs.-murtree" class="section level1">
<h1><strong>DL8.5 vs.Â MurTree</strong></h1>
<p>DL8.5 major difference between the MurTree algorithm is that the
number of feature nodes cannot be limited which means that full binary
trees are evaluated. To ensure a fair comparison with the feature node
selection strategy mentioned was set to maximum value. It is to be noted
that the authors compared the methods using runtime instead of
out-of-sample accuracy since the objective function is the same for both
the MurTree and DL8.5 methods solving the same problem. Results showed
that the MurTree algorithm was the fastest which is significant compared
to DL8.5 since it outruns previously mentioned methods. This is due to
the Mur tree employing additional techniques to further take advantage
of the properties of decision trees instead of using off-the-shelf tools
such as DL8.5.</p>
</div>
<div id="hyper-parameter-tuning-for-cart" class="section level1">
<h1><strong>Hyper Parameter Tuning for CART</strong></h1>
<p>Hyper-parameter tuning is what was used in class when conducting a
simple classification tree while performing a stratified 5-fold
cross-validation of a test/train set to obtain the accuracies. The model
is trained on training sets and evaluated on test sets. I have attached
a snippet of code used previously in our class to demonstrate how to
properly compute for the 5-fold cross validation.</p>
<pre class="r"><code>num_folds&lt;-5
folds&lt;-createFolds(train_data$SeriousInfection,k=num_folds,list = TRUE, returnTrain = FALSE)
crossval_errors&lt;-matrix(NA,nrow=length(cp_values),ncol=5)
for(i in 1:length(cp_values)){
# Split data into stratified folds
  for (j in 1:num_folds) {
  train_set&lt;-train_data[folds[[j]],]
  validation_set&lt;-train_data[-folds[[j]],]
  #Extract class weights for the current fold
  weights &lt;- class_weights[folds[[j]]]
  rpart1_model&lt;-rpart(SeriousInfection ~ ., data = train_set, weights = weights,
  method = &quot;class&quot;,cp=cp_values[i])
  predictions_tree&lt;-predict(rpart1_model, newdata=validation_set, type=&quot;class&quot;)
  crossval_errors[i,j]&lt;-mean(predictions_tree!=validation_set$SeriousInfection)
  }
}
crossval_errors_mean&lt;-rowMeans(crossval_errors)
#print(crossval_errors_mean)
# Find the index of the minimum error
min_error_index &lt;- which.min(crossval_errors_mean)

optimal_cp &lt;- cp_values[min_error_index]</code></pre>
<p>The classification tree printed below used from a previous example in
class in which we used cross validation for tree pruning.</p>
<pre class="r"><code>tree_model_prune&lt;-rpart(SeriousInfection ~.,weights=class_weights,data=train_data,method=&quot;class&quot;,cp=optimal_cp)
#saveRDS(tree_model_prune,file=&quot;tree_model_prune.rds&quot;)
tree_model_prune&lt;- readRDS(&quot;tree_model_prune.rds&quot;)
#plot(tree_model_prune)
rpart.plot(tree_model_prune)</code></pre>
<p><img src="Comparisons_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div id="heuristic-decision-trees-vs.-murtree" class="section level2">
<h2><strong>Heuristic Decision Trees vs.Â MurTree </strong></h2>
<p>When comparing heuristic decision trees to optimal ones with various
parameter settings, certain trends emerge. Initially, the optimal trees
often display superior training accuracy, although with similar
performance on the test set. This discrepancy suggests that the tree
structure derived from CART (Classification and Regression Trees) may
not be optimal for the dataset. Alternatively, heuristic decision trees
offer a benchmark for parameter tuning, providing an upper bound on
permissible parameter values based on CART-derived trees. However, fully
exploiting MurTree algorithm parameters, particularly up to depth four,
yields more promising results. This strategy demonstrates better
training accuracy, while considering all parameter options consistently
leads to superior out-of-sample accuracy across datasets. In summary,
while heuristic decision trees are faster, the MurTree algorithm
provides better generalization.</p>
</div>
</div>
<div id="random-forests-and-murtree" class="section level1">
<h1><strong>Random Forests and MurTree</strong></h1>
<p>Random forests and optimal decision trees were tuned the same to make
comparisons. While a forest of trees tends to be more accurate than a
single decision tree, it is also less concise and harder for human
interpretation. The random forests were tuned by adjusting for the
number of trees, maximum depth, and a subset of features considered at
each step. The results show that random forests generally outperform
optimal decision trees on both training and test sets. However optimal
decision trees of depth four achieve comparable performance in terms of
accuracy. This suggests that optimal decision trees may be preferred for
some applications over heuristically trained random forests. It is noted
that different tuning strategies for random forests may result in lower
or higher runtimes.</p>
<p>Just as the paper states that random forests are generally more
accurate but less concise and harder to interpret compared to single
decision trees. The provided random forest results below, showcasing a
classification type model, align with this expectation, demonstrating a
reasonably low out-of-bag (OOB) error rate of 8.5%.</p>
<pre class="r"><code># Separate your data into predictors (X) and the target variable (y)
X &lt;- core_data[, -1]  # Exclude the target variable column
y &lt;- core_data[[1]]   # Include only the target variable column


# Create the random forest model
rf_model &lt;- randomForest(x = X, y = y)

# Print a summary of the model
print(rf_model)</code></pre>
<pre><code>## 
## Call:
##  randomForest(x = X, y = y) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 5
## 
##         OOB estimate of  error rate: 8.5%
## Confusion matrix:
##      0 1  class.error
## 0 7232 2 0.0002764722
## 1  670 4 0.9940652819</code></pre>
<pre class="r"><code># Optionally, make predictions with the model
# Replace &#39;new_data&#39; with your new dataset for prediction
 predicted_values &lt;- predict(rf_model, newdata = core_data)</code></pre>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
