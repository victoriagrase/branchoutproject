<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Journal.knit</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Optimization Trees Review</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">About Decision Trees</a>
</li>
<li>
  <a href="Journal.html">MurTree </a>
</li>
<li>
  <a href="Comparisons.html">MurTree vs. Others</a>
</li>
<li>
  <a href="Links.html">Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<body style="background-color:aliceblue;">
<div id="review-of-murtree-optimal-decision-trees"
class="section level1">
<h1><strong>Review of MurTree: Optimal Decision Trees</strong></h1>
<p>The Murtree Algorithm computes optimal classification trees by
exhaustive search. The search space is exponentially large, but special
measures are taken to efficiently iterate through all trees, exploit the
overlap between trees, and avoid computing suboptimal decision
trees.</p>
</div>
<div id="core-equation" class="section level1">
<h1><strong>Core Equation</strong></h1>
<p>The following equation is the core foundation of the algorithm. The
input parameters include: the dataset <span
class="math inline">\(D\)</span> , <span
class="math inline">\(F\)</span> features (a variable that encodes
information about an object ), an upper bound on depth <span
class="math inline">\(d\)</span>, and an upper bound on the number of
feature nodes <span class="math inline">\(n\)</span> [1]. The output is
the minimum number of misclassifications. The first and second case
places constraints on the <span class="math inline">\(n\)</span> and
<span class="math inline">\(d\)</span> to avoid redundancy. For the
first case, if the specified upper bound on the number of feature nodes
<span class="math inline">\(n\)</span> is greater than <span
class="math inline">\(2^d-1\)</span>, where <span
class="math inline">\(d\)</span> is the upper bound on the depth of the
decision tree, then the function recursively call itself with <span
class="math inline">\(n\)</span> replaced by <span
class="math inline">\(2^d-1\)</span>. The maximum number of nodes in a
binary tree of depth <span class="math inline">\(d\)</span> is <span
class="math inline">\(2^d-1\)</span>, as each node in the tree has at
most two child nodes. So, if the specified upper bound on the number of
feature nodes exceeds the maximum number of nodes possible at depth
<span class="math inline">\(d\)</span>, the function adjusts <span
class="math inline">\(n\)</span> to <span
class="math inline">\(2^d-1\)</span> to adhere to this constraint. For
the second case, if the specified upper bound on the depth of the
decision tree <span class="math inline">\(d\)</span> is greater than the
number of feature nodes <span class="math inline">\(n\)</span>, then the
function resets the depth limit to be equal to the limit on the number
of feature nodes, ensuring that the depth of the decision tree does not
exceed the maximum number of nodes specified.</p>
<p>The third case captures the situation where the node must be declared
as a classification node, i.e., the node is labelled according to the
majority class [1]. Here, if upper bound on the number of feature nodes
<span class="math inline">\(n\)</span> is equal to the depth of the
decision tree <span class="math inline">\(d\)</span> or the upper bound
on the depth of the decision tree <span class="math inline">\(d\)</span>
is equal to zero, then the function returns the minimum of the
cardinality (the total number of unique elements in a set) of the
positive class instances <span class="math inline">\(|D^+|\)</span> and
the negative class instances <span class="math inline">\(|D^-|\)</span>
in the dataset <span class="math inline">\(D\)</span>. The fourth case
involves a recursive calculation of the minimum misclassification score
among all possible combinations of splitting a node into two child
nodes, considering different features <span
class="math inline">\(f\)</span> and different number of feature nodes
<span class="math inline">\(i\)</span> from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(n-1\)</span></p>
<p>For each feature <span class="math inline">\(f\)</span> in the set
<span class="math inline">\(F\)</span>, and for each possible number of
feature nodes <span class="math inline">\(i\)</span> from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(n-1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(T(D(f), d-1,i)\)</span> computes the
misclassification score when splitting the dataset <span
class="math inline">\(D\)</span> using feature <span
class="math inline">\(f\)</span> into two subsets and recursively
building a decision tree of depth <span
class="math inline">\(d-1\)</span> with <span
class="math inline">\(i\)</span> feature nodes in one of the child
nodes.</p></li>
<li><p><span class="math inline">\(T(D(\bar{f}), d-1,n-i-1)\)</span>
computes the misclassification score when not using feature <span
class="math inline">\(f\)</span> ( <span
class="math inline">\(\bar{f}\)</span> ) for splitting, recursively
building a decision tree of depth <span
class="math inline">\(d-1\)</span> with <span
class="math inline">\(n-i-1\)</span> feature nodes in one of the child
nodes.</p></li>
<li><p>The total misclassification score is the sum score of splitting
at the current node using feature <span class="math inline">\(f\)</span>
and not using feature <span class="math inline">\(f\)</span>,
respectively.</p></li>
</ul>
<p>The min function then selects the combination of feature and number
of feature nodes that results in the minimum total cost among all
possible combinations.</p>
<style>
.image1{
   position: center;
    width: 600px;
    height: 120px;
}
</style>
<p><img
src="images/mainequation.png"
class="image1" /></p>
</div>
<div id="algorithm-1" class="section level1">
<h1><strong>Algorithm 1</strong></h1>
<p>The input parameters include: a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes <span
class="math inline">\(n\)</span>. Additionally, an input parameter for
an upper bound that represents a limit on the number of
misclassification before the tree is deemed infeasible, i.e., not of
interest for example since a better tree is known [1]. The output is an
optimal classification tree considering the input constraints or
indication that no such tree exists (infeasible).</p>
<p>In lines 2-8, the algorithm starts by checking if the upper bound
<span class="math inline">\(UB\)</span> is negative. Furthermore, it
checks if the misclassification score of <span
class="math inline">\(T\)</span> exceeds the upper bound <span
class="math inline">\(UB\)</span> given that depth <span
class="math inline">\(d\)</span> or number of feature nodes <span
class="math inline">\(n\)</span> equals <span
class="math inline">\(0\)</span>. If so, infeasibility is returned.</p>
<p>In lines 9-14, the algorithm then checks if the optimal subtree is in
the cache. The cache includes information (the lower bound and/or the
optimal root node of the subtree under constraints on the depth and
number of feature nodes, which includes the root feature, the number of
feature nodes in its left and right children, and the misclassification
score) about the computed optimal subtree. If the optimal subtree is in
the cache, it is retrieved and stored in <span
class="math inline">\(T\)</span>, the algorithm checks if the
misclassification score of the optimal subtree <span
class="math inline">\(T\)</span> is lower than or equal to the upper
bound <span class="math inline">\(UB\)</span> . If this is the case,
then <span class="math inline">\(T\)</span> is returned as the optimal
subtree, otherwise infeasibility is reported.</p>
<p>In line 15, the algorithm updates the cache using similarity-based
lower bound <span
class="math inline">\(UpdateCacheUsingSimilarity(D,B,d,n)\)</span>
assuming that the optimal subtree is not in the cache. The optimal
solution may be found in the process. Here, given a dataset <span
class="math inline">\(D_{new}\)</span> for a node, the method helps to
derive a lower bound by considering the previously computed optimal
decision tree using the <span class="math inline">\(D_{old}\)</span> .
It considers the difference in the number of instances between <span
class="math inline">\(D_{old}\)</span> and <span
class="math inline">\(D_{new}\)</span> . Given the limit on the depth
<span class="math inline">\(d\)</span> and number of feature nodes <span
class="math inline">\(n\)</span>, a dataset <span
class="math inline">\(D_{new}\)</span> , and the dataset <span
class="math inline">\(D_{old}\)</span> with <span
class="math inline">\(T(D_{old},d,n)\)</span> as the misclassification
of <span class="math inline">\(D_{old}\)</span>. The similarity-based
lower bound is defined as,</p>
<p><span class="math display">\[
LB(D_{new},D_{old},d,n)=T(D_{old,d,n})-|D_{out}|, \hspace{3cm} [1]
\]</span></p>
<p>which is a lower bound which is a lower bound for the number of
misclassifications of the optimal decision tree for the dataset <span
class="math inline">\(D_{new}\)</span> of a tree depth <span
class="math inline">\(d\)</span> with <span
class="math inline">\(n\)</span> feature nodes [1].</p>
<p>In lines 16-21, the algorithm checks if the optimal solution (<span
class="math inline">\(updated\_optimal\_solution\)</span>) was found in
the process, if so it checks if the misclassification score is lower
than or equal to the upper bound <span
class="math inline">\(UB\)</span>. If so, then the optimal decision tree
<span class="math inline">\(T\)</span> is returned. Otherwise,
infeasibility is reported.</p>
<p>In lines 22-26, the algorithm attempts to prune based on the updated
lower bound stored in the cache. It retrieves the lower bound where
<span class="math inline">\(LB\)</span> is the <span
class="math inline">\(RetrieveLowerBoundFromCache(D,B,d,n)\)</span> and
checks if the updated lower bound <span
class="math inline">\(LB\)</span> is greater than the upper bound <span
class="math inline">\(UB\)</span>. If so, infeasibility is reported. If
<span class="math inline">\(LB\)</span> is equal to the
misclassification score of the classification node, a classification
node is returned.</p>
<p>In lines 27-32, the algorithm checks if the depth <span
class="math inline">\(d\)</span> is less than or equal to 2. If so then,
the specialized algorithm for tree of depth two (explained in Algorithm
4) (<span
class="math inline">\(SpecialisedDepthTwoAlgorithm(D,B,d,n)\)</span>) is
implemented. If the misclassification score from the optimal tree <span
class="math inline">\(T\)</span> is less than or equal to the upper
bound <span class="math inline">\(UB\)</span>, then <span
class="math inline">\(T\)</span> is returned as the optimal decision
tree. Otherwise, infeasibility is reported.</p>
<p>In line 33, the algorithm considers the general case (Core Equation
case 4), where the algorithm exhaustively explores the search space
through overlapping recursions [1]. This is detailed in Algorithm 2.</p>
<style>
.image1{
   position: center;
    width: 600px;
    height: 120px;
}
</style>
<img
src="images/algrotihm1description.png"
class="image1" />
<style>
.image3{
   position: center;
   left: 50px;
    width: 500px;
    height: 500px;
}
</style>
<p><img
src="images/algorithm1.png"
class="image3" /></p>
</div>
<div id="algorithm-2" class="section level1">
<h1><strong>Algorithm 2</strong></h1>
<p>The general case of the core equation used in the main algorithm
(Algorithm 1) is implemented if none of the conditions took place in
Algorithm 1. Similar to Algorithm 1, the input parameters include: a
data set <span class="math inline">\(D\)</span> consisting of positive
<span class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes <span
class="math inline">\(n\)</span>. Additionally, an input parameter for
an upper bound that represents a limit on the number of
misclassification before the tree is deemed infeasible, i.e., not of
interest for example since a better tree is known [1]. The output is an
optimal classification tree considering the input constraints or
indication that no such tree exists (infeasible).</p>
<p>In lines 2-4, the algorithm uses a single classification node as the
initial <span class="math inline">\(T_{best}\)</span> solution. If the
misclassification score of the classification node is greater than the
upper bound <span class="math inline">\(UB\)</span> then the initial
solution <span class="math inline">\(T_{best}\)</span> single
classification node is reported as infeasible.</p>
<p>In lines 5, the lower bound <span class="math inline">\(LB\)</span>
is retrieved from the cache</p>
<p>In line 6, the refined lower bound <span
class="math inline">\(RLB\)</span> is defined. The refined lower bound
<span class="math inline">\(RLB\)</span> is defined as,</p>
<p><span class="math display">\[
LB_{local}(D,f,d,n_{left},n_{right})=LB(D(\bar{f},d-1,n_{left}))+LB(D(f,d-1,n_{right}))
\hspace{2cm} \]</span> <span class="math display">\[
RLB(D,d,n)=\min{\{LB_{local}(D,f,d,n_{left},n_{right})|f \epsilon F
\land n_{left}+n_{right}=n-1 \}} \hspace{2cm} [1] \]</span></p>
<p>Here bound <span class="math inline">\(RLB\)</span> considers all
possible assignments of features and numbers of feature nodes to the
root and its children, and selects the minimum sum of the lower bounds
of its children [1].</p>
<p>In lines 7-8, the algorithm computes the allowed number of nodes for
child subtrees. In line 7 where <span class="math inline">\(n_{max}
\leftarrow min{\{2^{(d-1)}-1,n-1\}}\)</span>, <span
class="math inline">\(2^{(d-1)}\)</span> calculates the maximum number
of possible feature nodes possible in the subtree with depth <span
class="math inline">\(d-1\)</span>. Recall from the core equation that a
decision tree with depth <span class="math inline">\(d-1\)</span> can
have at most <span class="math inline">\(2^{d-1}\)</span> nodes and the
<span class="math inline">\(-1\)</span> accounts for the root node.
Here, <span class="math inline">\(n-1\)</span> is the upper bound on the
number of feature nodes. The minimum of these two values is selected. In
line 8, <span class="math inline">\(n_{min} \leftarrow
(n-1-n_{max})\)</span> calculates the remaining number of feature nodes
after accounting the maximum allowed nodes (<span
class="math inline">\(n_{max}\)</span> ).</p>
<p>For line 9, the algorithm begins the for loop for splitting feature
<span class="math inline">\(f\)</span> in the set <span
class="math inline">\(F\)</span>.</p>
<p>In lines 10-11, the algorithm checks if the current best
classification node <span class="math inline">\(T_{best}\)</span> is the
optimal node by confirming if it is equal to the lower bound <span
class="math inline">\(LB\)</span>. If so, the loop stops as the optimal
decision tree has been found.</p>
<p>In lines 12-13, attempt to ensure that nondiscriminary splits are
avoided. The condition if <span class="math inline">\(|D(\bar{f})|=0
\lor |D(f)|=0\)</span> implies that after splitting the dataset <span
class="math inline">\(D\)</span> based on feature <span
class="math inline">\(f\)</span>, one of the subsets is empty. This
means that the feature <span class="math inline">\(f\)</span> does not
provide any discriminatory power for separating the classes in the
dataset or, does not help to separating the classes effectively. These
splits do not provide useful information for classification. By
excluding these non-discriminatory splits, the decision tree becomes
more efficient and less prone to overfitting.</p>
<p>In line 14, the algorithm considers all possible combinations of
distributing the remaining node budget amongst its left <span
class="math inline">\(n_L\)</span> and right <span
class="math inline">\(n_R\)</span> subtrees. A nested for loop begins
for each feature split where the <span
class="math inline">\(n_L\)</span> values are in the interval <span
class="math inline">\([n_{min}, n_{max}]\)</span> .</p>
<p>In lines 15-17, the algorithm <span class="math inline">\(n_{R}
\leftarrow n-1-n_{L}\)</span> calculates the difference between the
total number of number of feature nodes and the number of feature nodes
on the left subtree and assigns it to <span
class="math inline">\(n_R\)</span> (this difference represents the
number of feature nodes available for use on the right subtree of the
split). An upper bound <span class="math inline">\(UB&#39;\)</span> is
then imposed by getting the minimum between the upper bound <span
class="math inline">\(UB\)</span> and the misclassifications score of
the best tree <span class="math inline">\(T_{best}\)</span> found thus.
Algorithm 3 is then used to compute the optimal tree given the feature
of the root and number of features in children subtrees (tree
configuration) or reports a lower local bound. In lines 18-21, if <span
class="math inline">\(T\)</span> is not considered infeasible then the
best tree <span class="math inline">\(T_{best}\)</span> is returned as
the optimal tree <span class="math inline">\(T\)</span>. Otherwise, the
refined lower bound <span class="math inline">\(RLB\)</span> is used by
getting the minimum between the refined lower bound and local lower
bound <span class="math inline">\(LB_{local}\)</span> . This is where
the nested for loop concludes</p>
<p>In lines 22-30, after exhausting all feature splits for the root
node, if the misclassification score of the best tree thus far <span
class="math inline">\(T_{best}\)</span> is lower than or equal to the
upper bound <span class="math inline">\(UB\)</span>, it is stored in
cache as the optimal subtree. Otherwise, the lower bound <span
class="math inline">\(LB\)</span> is computed and stored in the cache.
The refined lower bound is combined with the upper bound to get a lower
bound for the instance where no optimal subtree with less than the upper
bound <span class="math inline">\(UB\)</span> could be found. This done
using:</p>
<p><span class="math display">\[ T(D,d,n) \geq max{\{RLB(D,d,n), UB+1\}}
\hspace{3cm} [1] \]</span></p>
<p>The optimal tree is then returned.</p>
<img
src="images/algorithm2.png"
class="image1" />
<style>
.image5{
   position: center;
   left: 50px;
    width: 550px;
    height: 600px;
}
</style>
<p><img
src="images/algorithm2description.png"
class="image5" /></p>
</div>
<div id="algorithm-3" class="section level1">
<h1><strong>Algorithm 3</strong></h1>
<p>This algorithm is a subroutine used in general case (Algorithm 2) to
compute the optimal tree given the tree configuration. The algorithm is
used a strategy that prioritizes the subtree with the greater
misclassification score of its leaf node as this subtree is more likely
to result in a tree with more misclassifications [1]. If one subtree has
a high misclassification score it increases the likelihood of pruning
the other sibling [1]. Dynamic ordering is used where the left subtree
is processed first.</p>
<p>The input parameters include: a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the root feature <span
class="math inline">\(f_{root}\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes in the
left and right subtree <span class="math inline">\(n_{L},n_{R}\)</span>.
Additionally, an input parameter for an upper bound that represents a
limit on the number of misclassification before the tree is deemed
infeasible, i.e., not of interest for example since a better tree is
known [1]. The output is an optimal classification tree with feature
<span class="math inline">\(f_{root}\)</span> as its root that considers
the input constraints or a lower bound on the misclassification score is
returned if no such tree exists.</p>
<p>In lines 2-4, the algorithm starts by obtaining the depths of the
children subtrees from feature nodes in the left and right subtrees. The
branches of the children subtrees <span class="math inline">\(B_{L},
B_{R}\)</span> are also obtained from the branch information and root
feature inputs.</p>
<p>In lines 5-7, the algorithm checks if the misclassification score of
the leaf node <span class="math inline">\(D(\bar{f}_{root})\)</span> is
greater than the misclassification score of the leaf node <span
class="math inline">\(D(f_{root})\)</span>. If so, then the upper bound
for the left subtree <span class="math inline">\(UB_{L}\)</span> is
computed first (due to dynamic ordering) and assigned by getting the
difference between the upper bound and the lower bound from the cache.
The optimal left tree is then stored in <span
class="math inline">\(T_{L}\)</span>.</p>
<p>In lines 8-10, the algorithm checks if the left subtree is
infeasible. If so, the local lower bound <span
class="math inline">\(LB_{local}\)</span> is computed and returned.
Recall from Algorithm 2 that the local lower bound is the sum of the
left and right subtree lower bounds for the number of misclassifications
of an optimal decision tree for <span
class="math inline">\(D,n,d\)</span>.</p>
<p>In lines 11-18, the upper bound for the right subtree <span
class="math inline">\(UB_{R}\)</span> is obtained by getting the
difference between the upper bound <span
class="math inline">\(UB\)</span> and the misclassification score of the
left optimal subtree <span class="math inline">\(T_{L}\)</span> . From
the pseudocode, we observe that it is not needed to compute <span
class="math inline">\(UB_R\)</span> using the cache if the left child is
infeasible. The optimal right subtree is then stored in <span
class="math inline">\(T_{R}\)</span>. If the <span
class="math inline">\(T_{R}\)</span> is feasible, a tree with including
the root feature <span class="math inline">\(f_{root}\)</span> and
optimal left and right children subtrees (<span
class="math inline">\(T_{L},T_{R}\)</span>) is stored in <span
class="math inline">\(T\)</span> . <span
class="math inline">\(T\)</span> is then returned as the optimal
solution along with the misclassification score of <span
class="math inline">\(T\)</span>. Otherwise, the local lower bound is
computed and returned.</p>
<p>In lines 19-20, the algorithm repeats the process for the right
subtree first. <img
src="images/algorithm3.png"
class="image3" /></p>
</div>
<div id="algorithm-4" class="section level1">
<h1><strong>Algorithm 4</strong></h1>
<p><img
src="images/algorithm4.png"
class="image3" /></p>
</div>
<div id="algorithm-5" class="section level1">
<h1><strong>Algorithm 5</strong></h1>
<style>
.image4{
   position: center;
    width: 600px;
    height: 160px;
}
</style>
<p><img
src="images/algorithm5.png"
class="image4" /></p>
</div>
<div id="algorithm-6" class="section level1">
<h1><strong>Algorithm 6</strong></h1>
<style>
.image2{
   position: center;
    width: 600px;
    height: 250px;
}
</style>
<p><img
src="images/algorithm6.png"
class="image2" /></p>
</div>
<div id="algorithm-7" class="section level1">
<h1><strong>Algorithm 7</strong></h1>
<p><img
src="images/algorithm7.png"
class="image2" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
