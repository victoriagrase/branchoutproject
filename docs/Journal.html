<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Journal.knit</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Optimization Trees Review</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">About Decision Trees</a>
</li>
<li>
  <a href="Journal.html">MurTree </a>
</li>
<li>
  <a href="Comparisons.html">MurTree vs. Others</a>
</li>
<li>
  <a href="Links.html">Resources</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<body style="background-color:aliceblue;">
<div id="review-of-murtree" class="section level1">
<h1><strong>Review of MurTree</strong></h1>
<p>The MurTree algorithm is presented in the sections below. Algorithm 1
(summarizes the algorithm), Algorithm 2 (general case when none of the
conditions in Algorithm 1 occur), and Algorithm 3 (subroutine to compute
the optimal tree for a given tree configuration) are part of the core
algorithm. Algorithm 4 (computes optimal tree of depth two with three
nodes), Algorithm 5 (hash is computed), Algorithm 6 (minimize the sparse
objective), and Algorithm 7 (computes tree with minimum
misclassifications when considering a lexicographical objective) are
extensions that use the core algorithm as a basis.</p>
</div>
<div id="core-equation" class="section level1">
<h1><strong>Core Equation</strong></h1>
<p>The following equation is the core foundation of the algorithm. The
input parameters include: the dataset <span
class="math inline">\(D\)</span> , <span
class="math inline">\(F\)</span> features (a variable that encodes
information about an object ), an upper bound on depth <span
class="math inline">\(d\)</span>, and an upper bound on the number of
feature nodes <span class="math inline">\(n\)</span> [1]. The output is
the minimum number of misclassifications. The first and second case
places constraints on the <span class="math inline">\(n\)</span> and
<span class="math inline">\(d\)</span> to avoid redundancy. For the
first case, if the specified upper bound on the number of feature nodes
<span class="math inline">\(n\)</span> is greater than <span
class="math inline">\(2^d-1\)</span>, where <span
class="math inline">\(d\)</span> is the upper bound on the depth of the
decision tree, then the function recursively call itself with <span
class="math inline">\(n\)</span> replaced by <span
class="math inline">\(2^d-1\)</span>. The maximum number of nodes in a
binary tree of depth <span class="math inline">\(d\)</span> is <span
class="math inline">\(2^d-1\)</span>, as each node in the tree has at
most two child nodes. So, if the specified upper bound on the number of
feature nodes exceeds the maximum number of nodes possible at depth
<span class="math inline">\(d\)</span>, the function adjusts <span
class="math inline">\(n\)</span> to <span
class="math inline">\(2^d-1\)</span> to adhere to this constraint. For
the second case, if the specified upper bound on the depth of the
decision tree <span class="math inline">\(d\)</span> is greater than the
number of feature nodes <span class="math inline">\(n\)</span>, then the
function resets the depth limit to be equal to the limit on the number
of feature nodes, ensuring that the depth of the decision tree does not
exceed the maximum number of nodes specified.</p>
<p>The third case captures the situation where the node must be declared
as a classification node, i.e., the node is labelled according to the
majority class [1]. Here, if upper bound on the number of feature nodes
<span class="math inline">\(n\)</span> is equal to the depth of the
decision tree <span class="math inline">\(d\)</span> or the upper bound
on the depth of the decision tree <span class="math inline">\(d\)</span>
is equal to zero, then the function returns the minimum of the
cardinality (the total number of unique elements in a set) of the
positive class instances <span class="math inline">\(|D^+|\)</span> and
the negative class instances <span class="math inline">\(|D^-|\)</span>
in the dataset <span class="math inline">\(D\)</span>. The fourth case
involves a recursive calculation of the minimum misclassification score
among all possible combinations of splitting a node into two child
nodes, considering different features <span
class="math inline">\(f\)</span> and different number of feature nodes
<span class="math inline">\(i\)</span> from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(n-1\)</span></p>
<p>For each feature <span class="math inline">\(f\)</span> in the set
<span class="math inline">\(F\)</span>, and for each possible number of
feature nodes <span class="math inline">\(i\)</span> from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(n-1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(T(D(f), d-1,i)\)</span> computes the
misclassification score when splitting the dataset <span
class="math inline">\(D\)</span> using feature <span
class="math inline">\(f\)</span> into two subsets and recursively
building a decision tree of depth <span
class="math inline">\(d-1\)</span> with <span
class="math inline">\(i\)</span> feature nodes in one of the child
nodes.</p></li>
<li><p><span class="math inline">\(T(D(\bar{f}), d-1,n-i-1)\)</span>
computes the misclassification score when not using feature <span
class="math inline">\(f\)</span> ( <span
class="math inline">\(\bar{f}\)</span> ) for splitting, recursively
building a decision tree of depth <span
class="math inline">\(d-1\)</span> with <span
class="math inline">\(n-i-1\)</span> feature nodes in one of the child
nodes.</p></li>
<li><p>The total misclassification score is the sum score of splitting
at the current node using feature <span class="math inline">\(f\)</span>
and not using feature <span class="math inline">\(f\)</span>,
respectively.</p></li>
</ul>
<p>The min function then selects the combination of feature and number
of feature nodes that results in the minimum total cost among all
possible combinations.</p>
<style>
.image1 {
   display: block;
   margin: auto;
   width: 600px;
   height: 120px;
}
</style>
<p><img
src="images/mainequation.png"
class="image1" /></p>
</div>
<div id="algorithm-1" class="section level1">
<h1><strong>Algorithm 1</strong></h1>
<p>The input parameters include: a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes <span
class="math inline">\(n\)</span>. Additionally, an input parameter for
an upper bound that represents a limit on the number of
misclassification before the tree is deemed infeasible, i.e., not of
interest for example since a better tree is known [1]. The output is an
optimal classification tree considering the input constraints or
indication that no such tree exists (infeasible).</p>
<p>In lines 2-8, the algorithm starts by checking if the upper bound
<span class="math inline">\(UB\)</span> is negative. Furthermore, it
checks if the misclassification score of <span
class="math inline">\(T\)</span> exceeds the upper bound <span
class="math inline">\(UB\)</span> given that depth <span
class="math inline">\(d\)</span> or number of feature nodes <span
class="math inline">\(n\)</span> equals <span
class="math inline">\(0\)</span>. If so, infeasibility is returned.</p>
<p>In lines 9-14, the algorithm then checks if the optimal subtree is in
the cache. The cache includes information (the lower bound and/or the
optimal root node of the subtree under constraints on the depth and
number of feature nodes, which includes the root feature, the number of
feature nodes in its left and right children, and the misclassification
score) about the computed optimal subtree. If the optimal subtree is in
the cache, it is retrieved and stored in <span
class="math inline">\(T\)</span>, the algorithm checks if the
misclassification score of the optimal subtree <span
class="math inline">\(T\)</span> is lower than or equal to the upper
bound <span class="math inline">\(UB\)</span> . If this is the case,
then <span class="math inline">\(T\)</span> is returned as the optimal
subtree, otherwise infeasibility is reported.</p>
<p>In line 15, the algorithm updates the cache using similarity-based
lower bound <span
class="math inline">\(UpdateCacheUsingSimilarity(D,B,d,n)\)</span>
assuming that the optimal subtree is not in the cache. The optimal
solution may be found in the process. Here, given a dataset <span
class="math inline">\(D_{new}\)</span> for a node, the method helps to
derive a lower bound by considering the previously computed optimal
decision tree using the <span class="math inline">\(D_{old}\)</span> .
It considers the difference in the number of instances between <span
class="math inline">\(D_{old}\)</span> and <span
class="math inline">\(D_{new}\)</span> . Given the limit on the depth
<span class="math inline">\(d\)</span> and number of feature nodes <span
class="math inline">\(n\)</span>, a dataset <span
class="math inline">\(D_{new}\)</span> , and the dataset <span
class="math inline">\(D_{old}\)</span> with <span
class="math inline">\(T(D_{old},d,n)\)</span> as the misclassification
of <span class="math inline">\(D_{old}\)</span>. The similarity-based
lower bound is defined as,</p>
<p><span class="math display">\[
LB(D_{new},D_{old},d,n)=T(D_{old,d,n})-|D_{out}|, \hspace{3cm} [1]
\]</span></p>
<p>which is a lower bound which is a lower bound for the number of
misclassifications of the optimal decision tree for the dataset <span
class="math inline">\(D_{new}\)</span> of a tree depth <span
class="math inline">\(d\)</span> with <span
class="math inline">\(n\)</span> feature nodes [1].</p>
<p>In lines 16-21, the algorithm checks if the optimal solution (<span
class="math inline">\(updated\_optimal\_solution\)</span>) was found in
the process, if so it checks if the misclassification score is lower
than or equal to the upper bound <span
class="math inline">\(UB\)</span>. If so, then the optimal decision tree
<span class="math inline">\(T\)</span> is returned. Otherwise,
infeasibility is reported.</p>
<p>In lines 22-26, the algorithm attempts to prune based on the updated
lower bound stored in the cache. It retrieves the lower bound where
<span class="math inline">\(LB\)</span> is the <span
class="math inline">\(RetrieveLowerBoundFromCache(D,B,d,n)\)</span> and
checks if the updated lower bound <span
class="math inline">\(LB\)</span> is greater than the upper bound <span
class="math inline">\(UB\)</span>. If so, infeasibility is reported. If
<span class="math inline">\(LB\)</span> is equal to the
misclassification score of the classification node, a classification
node is returned.</p>
<p>In lines 27-32, the algorithm checks if the depth <span
class="math inline">\(d\)</span> is less than or equal to 2. If so then,
the specialized algorithm for tree of depth two (explained in Algorithm
4) (<span
class="math inline">\(SpecialisedDepthTwoAlgorithm(D,B,d,n)\)</span>) is
implemented. If the misclassification score from the optimal tree <span
class="math inline">\(T\)</span> is less than or equal to the upper
bound <span class="math inline">\(UB\)</span>, then <span
class="math inline">\(T\)</span> is returned as the optimal decision
tree. Otherwise, infeasibility is reported.</p>
<p>In line 33, the algorithm considers the general case (Core Equation
case 4), where the algorithm exhaustively explores the search space
through overlapping recursions [1]. This is detailed in Algorithm 2.</p>
<style>
.image1 {
   display: block;
   margin: auto;
   width: 600px;
   height: 120px;
}
</style>
<p><img
src="images/algrotihm1description.png"
class="image1" /></p>
<html>
<head>
<style>
        .image {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 650px;
            height: 650px;
        }
    </style>
</head>
<body>
<!-- Image -->
<p><img src="images/algorithm1.png" alt="Image" class="image"></p>
</div>
<div id="algorithm-2" class="section level1">
<h1><strong>Algorithm 2</strong></h1>
<p>The general case of the core equation used in the Algorithm 1 is
implemented if none of the conditions took place in Algorithm 1. Similar
to Algorithm 1, the input parameters include: a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes <span
class="math inline">\(n\)</span>. Additionally, an input parameter for
an upper bound that represents a limit on the number of
misclassification before the tree is deemed infeasible, i.e., not of
interest for example since a better tree is known [1]. The output is an
optimal classification tree considering the input constraints or
indication that no such tree exists (infeasible).</p>
<p>In lines 2-4, the algorithm uses a single classification node as the
initial <span class="math inline">\(T_{best}\)</span> solution. If the
misclassification score of the classification node is greater than the
upper bound <span class="math inline">\(UB\)</span> then the initial
solution <span class="math inline">\(T_{best}\)</span> single
classification node is reported as infeasible.</p>
<p>In lines 5, the lower bound <span class="math inline">\(LB\)</span>
is retrieved from the cache</p>
<p>In line 6, the refined lower bound <span
class="math inline">\(RLB\)</span> is defined. The refined lower bound
<span class="math inline">\(RLB\)</span> is defined as,</p>
<p><span class="math display">\[
LB_{local}(D,f,d,n_{left},n_{right})=LB(D(\bar{f},d-1,n_{left}))+LB(D(f,d-1,n_{right}))
\hspace{2cm} \]</span> <span class="math display">\[
RLB(D,d,n)=\min{\{LB_{local}(D,f,d,n_{left},n_{right})|f \epsilon F
\land n_{left}+n_{right}=n-1 \}} \hspace{2cm} [1] \]</span></p>
<p>Here bound <span class="math inline">\(RLB\)</span> considers all
possible assignments of features and numbers of feature nodes to the
root and its children, and selects the minimum sum of the lower bounds
of its children [1].</p>
<p>In lines 7-8, the algorithm computes the allowed number of nodes for
child subtrees. In line 7 where <span class="math inline">\(n_{max}
\leftarrow min{\{2^{(d-1)}-1,n-1\}}\)</span>, <span
class="math inline">\(2^{(d-1)}\)</span> calculates the maximum number
of possible feature nodes possible in the subtree with depth <span
class="math inline">\(d-1\)</span>. Recall from the core equation that a
decision tree with depth <span class="math inline">\(d-1\)</span> can
have at most <span class="math inline">\(2^{d-1}\)</span> nodes and the
<span class="math inline">\(-1\)</span> accounts for the root node.
Here, <span class="math inline">\(n-1\)</span> is the upper bound on the
number of feature nodes. The minimum of these two values is selected. In
line 8, <span class="math inline">\(n_{min} \leftarrow
(n-1-n_{max})\)</span> calculates the remaining number of feature nodes
after accounting the maximum allowed nodes (<span
class="math inline">\(n_{max}\)</span> ).</p>
<p>For line 9, the algorithm begins the for loop for splitting feature
<span class="math inline">\(f\)</span> in the set <span
class="math inline">\(F\)</span>.</p>
<p>In lines 10-11, the algorithm checks if the current best
classification node <span class="math inline">\(T_{best}\)</span> is the
optimal node by confirming if it is equal to the lower bound <span
class="math inline">\(LB\)</span>. If so, the loop stops as the optimal
decision tree has been found.</p>
<p>In lines 12-13, attempt to ensure that nondiscriminary splits are
avoided. The condition if <span class="math inline">\(|D(\bar{f})|=0
\lor |D(f)|=0\)</span> implies that after splitting the dataset <span
class="math inline">\(D\)</span> based on feature <span
class="math inline">\(f\)</span>, one of the subsets is empty. This
means that the feature <span class="math inline">\(f\)</span> does not
provide any discriminatory power for separating the classes in the
dataset or, does not help to separating the classes effectively. These
splits do not provide useful information for classification. By
excluding these non-discriminatory splits, the decision tree becomes
more efficient and less prone to overfitting.</p>
<p>In line 14, the algorithm considers all possible combinations of
distributing the remaining node budget amongst its left <span
class="math inline">\(n_L\)</span> and right <span
class="math inline">\(n_R\)</span> subtrees. A nested for loop begins
for each feature split where the <span
class="math inline">\(n_L\)</span> values are in the interval <span
class="math inline">\([n_{min}, n_{max}]\)</span> .</p>
<p>In lines 15-17, the algorithm <span class="math inline">\(n_{R}
\leftarrow n-1-n_{L}\)</span> calculates the difference between the
total number of number of feature nodes and the number of feature nodes
on the left subtree and assigns it to <span
class="math inline">\(n_R\)</span> (this difference represents the
number of feature nodes available for use on the right subtree of the
split). An upper bound <span class="math inline">\(UB&#39;\)</span> is
then imposed by getting the minimum between the upper bound <span
class="math inline">\(UB\)</span> and the misclassifications score of
the best tree <span class="math inline">\(T_{best}\)</span> found thus.
Algorithm 3 is then used to compute the optimal tree given the feature
of the root and number of features in children subtrees (tree
configuration) or reports a lower local bound. In lines 18-21, if <span
class="math inline">\(T\)</span> is not considered infeasible then the
best tree <span class="math inline">\(T_{best}\)</span> is returned as
the optimal tree <span class="math inline">\(T\)</span>. Otherwise, the
refined lower bound <span class="math inline">\(RLB\)</span> is used by
getting the minimum between the refined lower bound and local lower
bound <span class="math inline">\(LB_{local}\)</span> . This is where
the nested for loop concludes</p>
<p>In lines 22-30, after exhausting all feature splits for the root
node, if the misclassification score of the best tree thus far <span
class="math inline">\(T_{best}\)</span> is lower than or equal to the
upper bound <span class="math inline">\(UB\)</span>, it is stored in
cache as the optimal subtree. Otherwise, the lower bound <span
class="math inline">\(LB\)</span> is computed and stored in the cache.
The refined lower bound is combined with the upper bound to get a lower
bound for the instance where no optimal subtree with less than the upper
bound <span class="math inline">\(UB\)</span> could be found. This done
using:</p>
<p><span class="math display">\[ T(D,d,n) \geq max{\{RLB(D,d,n), UB+1\}}
\hspace{3cm} [1] \]</span></p>
<p>The optimal tree is then returned.</p>
<p><img
src="images/algorithm2.png"
class="image1" /></p>
<html>
<head>
<style>
        .image5{
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 650px;
            height: 680px;
        }
    </style>
</head>
<body>
<p><img
src="images/algorithm2description.png"
class="image5" /></p>
</div>
<div id="algorithm-3" class="section level1">
<h1><strong>Algorithm 3</strong></h1>
<p>This algorithm is a subroutine used in general case (Algorithm 2) to
compute the optimal tree given the tree configuration. The algorithm is
used a strategy that prioritizes the subtree with the greater
misclassification score of its leaf node as this subtree is more likely
to result in a tree with more misclassifications [1]. If one subtree has
a high misclassification score it increases the likelihood of pruning
the other sibling [1]. Dynamic ordering is used where the left subtree
is processed first.</p>
<p>The input parameters include: a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances, branch information of the
subtree <span class="math inline">\(B\)</span>, the root feature <span
class="math inline">\(f_{root}\)</span>, the depth <span
class="math inline">\(d\)</span>, the number of feature nodes in the
left and right subtree <span class="math inline">\(n_{L},n_{R}\)</span>.
Additionally, an input parameter for an upper bound that represents a
limit on the number of misclassification before the tree is deemed
infeasible, i.e., not of interest for example since a better tree is
known [1]. The output is an optimal classification tree with feature
<span class="math inline">\(f_{root}\)</span> as its root that considers
the input constraints or a lower bound on the misclassification score is
returned if no such tree exists.</p>
<p>In lines 2-4, the algorithm starts by obtaining the depths of the
children subtrees from feature nodes in the left and right subtrees. The
branches of the children subtrees <span class="math inline">\(B_{L},
B_{R}\)</span> are also obtained from the branch information and root
feature inputs.</p>
<p>In lines 5-7, the algorithm checks if the misclassification score of
the leaf node <span class="math inline">\(D(\bar{f}_{root})\)</span> is
greater than the misclassification score of the leaf node <span
class="math inline">\(D(f_{root})\)</span>. If so, then the upper bound
for the left subtree <span class="math inline">\(UB_{L}\)</span> is
computed first (due to dynamic ordering) and assigned by getting the
difference between the upper bound and the lower bound from the cache.
The optimal left tree is then stored in <span
class="math inline">\(T_{L}\)</span>.</p>
<p>In lines 8-10, the algorithm checks if the left subtree is
infeasible. If so, the local lower bound <span
class="math inline">\(LB_{local}\)</span> is computed and returned.
Recall from Algorithm 2 that the local lower bound is the sum of the
left and right subtree lower bounds for the number of misclassifications
of an optimal decision tree for <span
class="math inline">\(D,n,d\)</span>.</p>
<p>In lines 11-18, the upper bound for the right subtree <span
class="math inline">\(UB_{R}\)</span> is obtained by getting the
difference between the upper bound <span
class="math inline">\(UB\)</span> and the misclassification score of the
left optimal subtree <span class="math inline">\(T_{L}\)</span> . From
the pseudocode, we observe that it is not needed to compute <span
class="math inline">\(UB_R\)</span> using the cache if the left child is
infeasible. The optimal right subtree is then stored in <span
class="math inline">\(T_{R}\)</span>. If the <span
class="math inline">\(T_{R}\)</span> is feasible, a tree with including
the root feature <span class="math inline">\(f_{root}\)</span> and
optimal left and right children subtrees (<span
class="math inline">\(T_{L},T_{R}\)</span>) is stored in <span
class="math inline">\(T\)</span> . <span
class="math inline">\(T\)</span> is then returned as the optimal
solution along with the misclassification score of <span
class="math inline">\(T\)</span>. Otherwise, the local lower bound is
computed and returned.</p>
<p>In lines 19-20, the algorithm repeats the process for the right
subtree first.</p>
<html>
<head>
<style>
        .image9{
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 700px;
            height: 650px;
        }
    </style>
</head>
<body>
<p><img
src="images/algorithm3.png"
class="image9" /></p>
</div>
<div id="algorithm-4" class="section level1">
<h1><strong>Algorithm 4</strong></h1>
<p>While this algorithm is considered an extension to the core
algorithm, it is implemented in Algorithm 1 for small tree instances
where optimal classification trees of depth two with three nodes are
computed. The input parameter is a data set <span
class="math inline">\(D\)</span> consisting of positive <span
class="math inline">\(D^+\)</span> and negative <span
class="math inline">\(D^-\)</span> instances.</p>
<p>In lines 2-3, the specialized algorithm begins by initializing
frequency counters for each individual feature ( <span
class="math inline">\(FQ^{+}(f_{i})\)</span> and <span
class="math inline">\(FQ^{-}(f_{i})\)</span> ) ,for positive and
negative instances. The frequency counters pairs of features <span
class="math inline">\(f_{i}\)</span> and <span
class="math inline">\(f_{j}\)</span> where <span
class="math inline">\(i\)</span> is less than <span
class="math inline">\(j\)</span> are also initialized ( <span
class="math inline">\(FQ +(f_{i}, f_{j})\)</span> and <span
class="math inline">\(FQ - (f_{i}, f_{j})\)</span> ), for positive and
negative instances.</p>
<p>In lines 4-9 (Step 1), the algorithm iterates through each positive
instance <span class="math inline">\(fv\)</span> in the dataset <span
class="math inline">\(D^{+}\)</span> . For each feature <span
class="math inline">\(f_{i}\)</span> in the instance, the frequency
counter <span class="math inline">\(FQ+ (F_{i})\)</span> is incremented.
For each pair of features <span class="math inline">\(f_{i}\)</span> and
<span class="math inline">\(f_{j}\)</span> (where <span
class="math inline">\(i\)</span> is less than <span
class="math inline">\(j\)</span>) in the instance, the frequency counter
<span class="math inline">\(FQ + (f_{i}, f_{j})\)</span> is incremented.
Frequency counters for negative instances <span
class="math inline">\(FQ-(f_{i})\)</span> and <span
class="math inline">\(FQ -(f_{i}, f_{j})\)</span> are also computed
using the dataset <span class="math inline">\(D^-\)</span>.</p>
<p>In lines 10-18 (Step 2), the algorithm computes the optimal decision
tree from the frequency counters <span
class="math inline">\(FQ^+\)</span> and <span
class="math inline">\(FQ^-\)</span>. It exploits the frequency counts to
efficiently enumerate decision trees without needing to explicitly refer
to the data [1]. This provides a substantial speed-up compared to
iterating through features and splitting data as given in the dynamic
programming formulation (Eq. 1) for decision trees of depth two [1]. Two
nested for loops iterate through each pair of features <span
class="math inline">\(f_i\)</span> and <span
class="math inline">\(f_j\)</span> , excluding cases where <span
class="math inline">\(i\)</span> is equal to <span
class="math inline">\(j\)</span> . The classification score for a
classification node with all instances of <span
class="math inline">\(D\)</span> containing both features <span
class="math inline">\(f_i\)</span> and <span
class="math inline">\(f_j\)</span> is then computed using,</p>
<p><span class="math display">\[ CS(f_{i},f_{j}) =
min{\{FQ^{+}(f_{i},f_{j}), FQ^{-}(f_{i},f_{j})\}} \hspace{3cm} [1]
\]</span></p>
<p>This is implemented in lines 12 and 13 where the classification
scores <span class="math inline">\(CS(\bar{f_{i}},f_{j})\)</span> and
<span class="math inline">\(CS(\bar{f_{i} },\bar{f_{j}})\)</span> are
computed which is the minimum of the frequency counts<span
class="math inline">\(FQ^{+}(\bar{f_{i}},f_{j})\)</span> and <span
class="math inline">\(FQ^{-}(\bar{f_{i}},f_{j})\)</span> and <span
class="math inline">\(FQ^{+}(\bar{f_{i}},\bar{f_{j}})\)</span> and <span
class="math inline">\(FQ^{-}(\bar{f_{i}},\bar{f_{j}})\)</span>,
respectively. The misclassification score of the left or right subtree
is computed using,</p>
<p><span class="math display">\[ MS_{left}(f_{left}) =
CS(\bar{f_{root}}, \bar{f_{left}})+ CS(\bar{f_{root}},f_{left})
\hspace{2cm}\]</span></p>
<p><span class="math display">\[ MS_{right}(f_{right}) =
CS(\bar{f_{root}}, \bar{f_{right}})+ CS(f_{root},f_{right}) \hspace{1cm}
[1] \]</span></p>
<p>This is implemented in line 14 where the misclassification score is
computed for the branch where feature <span
class="math inline">\(f_{i}\)</span> is the root and <span
class="math inline">\(f_j\)</span> is the left child. In lines 15-18,
the algorithm checks if the misclassification score of the current left
subtree is greater than the computed misclassification score <span
class="math inline">\(MS_{left}(f_{i},f_{j})\)</span>. If so, then <span
class="math inline">\(MS_{left}(f_{i},f_{j})\)</span> is assigned as the
best left subtree. The updated feature of the best left subtree is
assigned to <span class="math inline">\(f_j\)</span>, indicating that
feature <span class="math inline">\(f_{j}\)</span> is the chosen feature
for the left child when feature <span class="math inline">\(f_i\)</span>
is the root. The best right subtree is then computed with <span
class="math inline">\(f_i\)</span> as the root and <span
class="math inline">\(f_{j}\)</span> as the right child using the same
process.</p>
<p>In lines 19 and 20, the best tree is computed by obtaining the root
feature <span class="math inline">\(f_i\)</span> that minimizes the sum
of misclassification scores of the best left and right subtrees. This is
returned as the optimal decision tree.</p>
<html>
<head>
<style>
        .image8{
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 600px;
            height: 650px;
        }
    </style>
</head>
<body>
<p><img
src="images/algorithm4.png"
class="image8" /></p>
</div>
<div id="algorithm-5" class="section level1">
<h1><strong>Algorithm 5</strong></h1>
<p>Algorithm 5 is one of the proposed extensions to the core algorithm
where the concept of subtree hashing based on branches and subtree
hashing based on datasets is explored. This algorithm aims to generate a
hash value for an array of integers by iteratively combining each
element of the array with the current hash value <span
class="math inline">\(k\)</span> using bitwise XOR and other operations
including introducing a constant ( <span
class="math inline">\(0x9e37796b9\)</span> ) often used in hash
functions for its properties and the expression <span
class="math inline">\(64k+k/4\)</span> to prevent simple patterns input
from propagating the output [1]. The resulting hash value is computed
based on the properties of the elements in the array and is used for
various purposes, such as indexing into hash tables or identifying
unique representations of data structures.</p>
<p>Each subtree in the decision tree is associated with exactly one
branch. Conversely, a single branch of length <span
class="math inline">\(k\)</span> may be linked to <span
class="math inline">\(k!\)</span> subtrees. This means that the
computation of a single subtree can be shared among all subtrees
associated with the same branch, which can improve efficiency [1].The
branch-based cache is described as an array of hash tables, where each
branch of length <span class="math inline">\(k\)</span> is stored in the
<span class="math inline">\(k\)</span>-th hash table. Branches are
stored as sorted arrays, with features converted into integers based on
their index and polarity. Specifically, positive features are assigned
the value <span class="math inline">\(2i+1\)</span> and negative
features are assigned the value <span class="math inline">\(2i\)</span>,
where <span class="math inline">\(i\)</span> is the index of the
feature. A conventional hash function is used on these integer arrays
within the hash tables to compute the hash of each branch. The hash
function described in Algorithm 5 computes the hash of an integer array
<span class="math inline">\(A\)</span> of length <span
class="math inline">\(n\)</span>.</p>
<p>The branch representation, while useful, has limitations in sharing
computed results only with subtrees that have equivalent branches. More
general representations allow sharing computed results with any subtree
that optimizes the same subproblem, addressing this limitation. The
proposed alternative representation uses the dataset itself as the
subproblem representation. Each instance in the dataset is assigned a
unique identifier in the form of an integer. Datasets are stored in an
array of hash tables, where datasets with <span
class="math inline">\(m\)</span> instances are stored in the <span
class="math inline">\(m\)</span>-th hash table. The hash value of a
dataset is computed using the instance identifiers and Algorithm 5,
treating the dataset as an array with instance identifiers as integers.
The computed hash is stored in the dataset for further use. Dataset
caching showed better performance than branch caching in experiments,
and additional memory requirements were not a significant issue [1].</p>
<style>
.image4 {
   display: block;
   margin: auto;
   width: 600px;
   height: 150px;
}
</style>
<p><img
src="images/algorithm5.png"
class="image4" /></p>
</div>
<div id="algorithm-6" class="section level1">
<h1><strong>Algorithm 6</strong></h1>
<p>Algorithm 6 is one of the proposed extensions to the core algorithm
to minimize the sparse objective. The sparse objective is defined as a
weighted linear combination of the misclassification score and the
number of feature nodes [1]. It is given by,</p>
<p><span class="math display">\[ misclassification + \alpha * nodes
\hspace{2cm} [1] \]</span>It is used to help balance the trade-off
between the size of the decision tree (controlled by the number of
nodes) and its predictive accuracy (measured by the misclassification
score). The sparse coefficient <span
class="math inline">\(\alpha\)</span> determines the importance of the
treeâ€™s size relative to its accuracy. Adding a node to the tree is
considered beneficial only if it leads to a reduction in
misclassifications by at least <span
class="math inline">\(\alpha\)</span>.</p>
<p>This is implemented in Algorithm 6 where the input parameters
include: dataset <span class="math inline">\(D\)</span>, depth <span
class="math inline">\(d\)</span>, number of feature nodes <span
class="math inline">\(n\)</span>, and sparse coefficient <span
class="math inline">\(\alpha\)</span>. The output is the optimal
decision tree that minimizes the sparse object on dataset <span
class="math inline">\(D\)</span>. In line 2, <span
class="math inline">\(T_{best}\)</span> is initialized as a
classification node based on dataset <span
class="math inline">\(D\)</span>. This serves as the starting point for
the best decision tree found so far. For the sparse objective
computation (lines 3 and 4), for each <span
class="math inline">\(n_0\)</span> in the range from 1 to <span
class="math inline">\(n\)</span>, the algorithm computes an upper bound
<span class="math inline">\(UB\)</span> for the sparse objective. The
upper bound is calculated as the difference between the sparse objective
of the current best tree and (<span
class="math inline">\(\alpha\)</span> * the current number of feature
nodes <span class="math inline">\(n_0\)</span>) -1. The optimal subtree
is then obtained. If the optimal tree is feasible, then the optimal
subtree is returned as the best decision tree.</p>
<p>NOTE: Running Algorithm 6 with <span
class="math inline">\(\alpha\)</span> =0 captures all possible trees
given the specified upper bounds on depth and number of nodes. This
computation is cached for immediate access, facilitating quick
determination of the optimal tree for any <span
class="math inline">\(\alpha\)</span> [1]. When tuning the sparse
coefficient <span class="math inline">\(\alpha\)</span> ; for deeper
trees, it may be more efficient to consider specific <span
class="math inline">\(\alpha\)</span> values rather than tuning for all
possible values. Setting <span class="math inline">\(\alpha\)</span>
&gt; 0 contributes to reducing the search space, potentially helping the
search process for deeper trees.</p>
<style>
.image2 {
   display: block;
   margin: auto;
   width: 600px;
   height: 250px;
}
</style>
<p><img
src="images/algorithm6.png"
class="image2" /></p>
</div>
<div id="algorithm-7" class="section level1">
<h1><strong>Algorithm 7</strong></h1>
<p>Algorithm 7 is one of the proposed extensions to the core algorithm
where a lexicographical objective is used to compute the tree with
minimum misclassifications using the least amount of nodes. The
lexicographical objective is taken into account by computing the optimal
tree first and then querying the algorithm to compute smaller trees
using the misclassification score as an upper bound [1].</p>
<p>This is implemented in Algorithm 7 where the input parameters
include: dataset <span class="math inline">\(D\)</span>, depth <span
class="math inline">\(d\)</span>, and the number of feature nodes <span
class="math inline">\(n\)</span>. The output is the optimal decision
tree that minimizes the misclassification and then the number of nodes
on the dataset <span class="math inline">\(D\)</span>. In line 2-3, the
best tree <span class="math inline">\(T_{best}\)</span> is initialized,
using the MurTree.SolveSubtree function. The misclassification score of
the current best tree <span class="math inline">\(T_{best}\)</span> is
then computed and assigned as the upper bound <span
class="math inline">\(UB\)</span>. In line 4, a for loop begins to
iterate over the number of feature nodes <span
class="math inline">\(n_{0}\)</span> from <span
class="math inline">\(n-1\)</span> to 0. For each <span
class="math inline">\(n_0\)</span>, the <span
class="math inline">\(MurTree.SolveSubtree\)</span> function is used to
find the optimal subtree <span class="math inline">\(T\)</span> with
<span class="math inline">\(D,d,n_{0}\)</span>, and the current upper
bound <span class="math inline">\(UB\)</span>. If the optimal subtree
<span class="math inline">\(T\)</span> is feasible then is it returned
as the best tree <span class="math inline">\(T_{best}\)</span>.</p>
<p><img
src="images/algorithm7.png"
class="image2" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
